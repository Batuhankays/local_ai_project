"""
RAG (Retrieval-Augmented Generation) Motoru

Bu modÃ¼l, vektÃ¶r veritabanÄ± kullanarak dokÃ¼man chunk'larÄ±nÄ± saklar,
sorgu embedding'i oluÅŸturur, en yakÄ±n chunk'larÄ± bulur ve
Ollama ile cevap Ã¼retir.
"""

import os
import pickle
from typing import List, Dict, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import ollama


class Embedder:
    """Metin embedding oluÅŸturma"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        """
        Args:
            model_name: Sentence-transformers model adÄ±
        """
        print(f"ğŸ¤– Embedding modeli yÃ¼kleniyor: {model_name}")
        self.model = SentenceTransformer(model_name)
        print("   âœ“ Model yÃ¼klendi")
    
    def encode(self, texts: List[str]) -> np.ndarray:
        """
        Metinleri embedding'e Ã§evir
        
        Args:
            texts: Metin listesi
        
        Returns:
            Embedding matrisi (n_texts x embedding_dim)
        """
        return self.model.encode(texts, show_progress_bar=True)
    
    def encode_single(self, text: str) -> np.ndarray:
        """Tek bir metni embedding'e Ã§evir"""
        return self.model.encode([text])[0]


class VectorStore:
    """FAISS tabanlÄ± vektÃ¶r veritabanÄ± (basitleÅŸtirilmiÅŸ)"""
    
    def __init__(self, embedding_dim: int = 384):
        """
        Args:
            embedding_dim: Embedding boyutu (all-MiniLM-L6-v2 iÃ§in 384)
        """
        self.embedding_dim = embedding_dim
        self.embeddings = []
        self.chunks = []
        self.index_built = False
    
    def add_documents(self, chunks: List[Dict], embeddings: np.ndarray):
        """
        DokÃ¼manlarÄ± ve embedding'leri ekle
        
        Args:
            chunks: Chunk metadata listesi
            embeddings: Chunk embedding'leri
        """
        if len(chunks) != len(embeddings):
            raise ValueError("Chunk sayÄ±sÄ± embedding sayÄ±sÄ±na eÅŸit olmalÄ±")
        
        self.chunks.extend(chunks)
        self.embeddings.append(embeddings)
        self.index_built = False
    
    def build_index(self):
        """Embedding matrisini oluÅŸtur"""
        if self.embeddings:
            self.embeddings = np.vstack(self.embeddings)
            self.index_built = True
            print(f"âœ“ VektÃ¶r indeksi oluÅŸturuldu: {len(self.chunks)} chunk")
    
    def search(self, query_embedding: np.ndarray, top_k: int = 3) -> List[Dict]:
        """
        En yakÄ±n chunk'larÄ± bul (cosine similarity)
        
        Args:
            query_embedding: Sorgu embedding'i
            top_k: KaÃ§ sonuÃ§ dÃ¶ndÃ¼rÃ¼lecek
        
        Returns:
            En yakÄ±n chunk'lar
        """
        if not self.index_built:
            self.build_index()
        
        if len(self.chunks) == 0:
            return []
        
        # Cosine similarity hesapla
        query_norm = query_embedding / np.linalg.norm(query_embedding)
        embeddings_norm = self.embeddings / np.linalg.norm(
            self.embeddings, axis=1, keepdims=True
        )
        
        similarities = np.dot(embeddings_norm, query_norm)
        
        # En yÃ¼ksek skorlarÄ± al
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            result = self.chunks[idx].copy()
            result['similarity'] = float(similarities[idx])
            results.append(result)
        
        return results
    
    def save(self, path: str):
        """VektÃ¶r DB'yi kaydet"""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        data = {
            'embeddings': self.embeddings,
            'chunks': self.chunks,
            'embedding_dim': self.embedding_dim
        }
        
        with open(path, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"ğŸ’¾ VektÃ¶r DB kaydedildi: {path}")
    
    def load(self, path: str):
        """VektÃ¶r DB'yi yÃ¼kle"""
        if not os.path.exists(path):
            raise FileNotFoundError(f"VektÃ¶r DB bulunamadÄ±: {path}")
        
        with open(path, 'rb') as f:
            data = pickle.load(f)
        
        self.embeddings = data['embeddings']
        self.chunks = data['chunks']
        self.embedding_dim = data['embedding_dim']
        self.index_built = True
        
        print(f"âœ“ VektÃ¶r DB yÃ¼klendi: {len(self.chunks)} chunk")


class OllamaLLM:
    """Ollama LLM entegrasyonu"""
    
    def __init__(self, model: str = "llama3.1:8b", url: str = "http://localhost:11434"):
        """
        Args:
            model: Ollama model adÄ± (llama3.1:8b Ã¶nerilen)
            url: Ollama API URL
        """
        self.model = model
        self.url = url
        self._check_connection()
    
    def _check_connection(self):
        """Ollama baÄŸlantÄ±sÄ±nÄ± kontrol et"""
        try:
            ollama.list()
            print(f"âœ“ Ollama baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ± (Model: {self.model})")
        except Exception as e:
            print(f"âš ï¸  Ollama baÄŸlantÄ± hatasÄ±: {e}")
            print("   Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun: ollama serve")
    
    def generate(
        self, 
        prompt: str, 
        system: str = "",
        temperature: float = 0.3,
        top_p: float = 0.9,
        max_tokens: int = 1024
    ) -> str:
        """
        Ollama ile cevap Ã¼ret
        
        Args:
            prompt: KullanÄ±cÄ± prompt'u
            system: Sistem prompt'u
            temperature: YaratÄ±cÄ±lÄ±k (0.0-1.0, dÃ¼ÅŸÃ¼k = daha deterministik)
            top_p: Nucleus sampling
            max_tokens: Maksimum token sayÄ±sÄ±
        
        Returns:
            Ãœretilen cevap
        """
        try:
            messages = []
            
            if system:
                messages.append({
                    'role': 'system',
                    'content': system
                })
            
            messages.append({
                'role': 'user',
                'content': prompt
            })
            
            response = ollama.chat(
                model=self.model,
                messages=messages,
                options={
                    'temperature': temperature,
                    'top_p': top_p,
                    'num_predict': max_tokens,
                }
            )
            
            return response['message']['content']
        
        except Exception as e:
            error_msg = str(e)
            if "404" in error_msg or "not found" in error_msg:
                return (f"Ollama hatasÄ±: model '{self.model}' not found (status code: 404)\n\n"
                       f"Model indirmek iÃ§in terminalde Ã§alÄ±ÅŸtÄ±rÄ±n:\n"
                       f"  ollama pull {self.model}\n\n"
                       f"Veya farklÄ± bir model kullanÄ±n (mistral, gemma2:9b vb.)")
            return f"Ollama hatasÄ±: {str(e)}\n\nOllama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan ve '{self.model}' modelinin yÃ¼klÃ¼ olduÄŸundan emin olun."


class RAGEngine:
    """RAG sistemi ana motoru"""
    
    def __init__(
        self,
        embedding_model: str = "all-MiniLM-L6-v2",
        llm_model: str = "mistral",
        vector_db_path: Optional[str] = None
    ):
        """
        Args:
            embedding_model: Sentence-transformers model
            llm_model: Ollama model
            vector_db_path: VektÃ¶r DB yolu (varsa yÃ¼kle)
        """
        self.embedder = Embedder(model_name=embedding_model)
        self.vector_store = VectorStore(embedding_dim=384)
        self.llm = OllamaLLM(model=llm_model)
        
        # VektÃ¶r DB varsa yÃ¼kle
        if vector_db_path and os.path.exists(vector_db_path):
            self.vector_store.load(vector_db_path)
    
    def add_documents(self, chunks: List[Dict]):
        """
        DokÃ¼manlarÄ± RAG sistemine ekle
        
        Args:
            chunks: Chunk listesi (document_processor'dan)
        """
        if not chunks:
            print("Eklenecek chunk yok")
            return
        
        print(f"\nğŸ”„ {len(chunks)} chunk iÃ§in embedding oluÅŸturuluyor...")
        
        # Embedding oluÅŸtur
        texts = [chunk['text'] for chunk in chunks]
        embeddings = self.embedder.encode(texts)
        
        # VektÃ¶r DB'ye ekle
        self.vector_store.add_documents(chunks, embeddings)
        self.vector_store.build_index()
    
    def retrieve_context(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        Sorguya en yakÄ±n dokÃ¼man parÃ§alarÄ±nÄ± getir
        
        Args:
            query: KullanÄ±cÄ± sorusu
            top_k: KaÃ§ chunk dÃ¶ndÃ¼rÃ¼lecek
        
        Returns:
            En yakÄ±n chunk'lar
        """
        # Query embedding oluÅŸtur
        query_embedding = self.embedder.encode_single(query)
        
        # Benzer chunk'larÄ± bul
        results = self.vector_store.search(query_embedding, top_k=top_k)
        
        return results
    
    def generate_answer(
        self,
        query: str,
        context_chunks: Optional[List[Dict]] = None,
        fault_info: Optional[Dict] = None,
        top_k: int = 3
    ) -> str:
        """
        Sorguya cevap Ã¼ret (RAG)
        
        Args:
            query: KullanÄ±cÄ± sorusu
            context_chunks: Ã–nceden alÄ±nmÄ±ÅŸ context (yoksa otomatik al)
            fault_info: ArÄ±za kodu bilgisi (varsa)
            top_k: KaÃ§ chunk kullanÄ±lacak
        
        Returns:
            Ãœretilen cevap
        """
        # Context yoksa al
        if context_chunks is None:
            context_chunks = self.retrieve_context(query, top_k=top_k)
        
        # System prompt - GeliÅŸmiÅŸ versiyon
        system_prompt = """Sen askeri jeneratÃ¶r bakÄ±m ve arÄ±za giderme konusunda uzman bir teknisyensin.

GÃ–REVÄ°N:
1. Verilen teknik dokÃ¼manlara SADECE dayanarak cevap ver
2. Emin olmadÄ±ÄŸÄ±n konularda "Bu bilgi dokÃ¼manlarÄ±mda yok" de
3. AdÄ±m adÄ±m, net ve uygulanabilir Ã§Ã¶zÃ¼mler sun
4. GÃ¼venlik Ã¶nlemleri varsa MUTLAKA belirt

CEVAP FORMATI:
- KÄ±sa Ã¶zet ile baÅŸla (1-2 cÃ¼mle)
- AdÄ±m adÄ±m Ã§Ã¶zÃ¼m sun (numaralÄ± liste)
- GÃ¼venlik uyarÄ±sÄ± varsa belirt
- Hangi dokÃ¼man/bÃ¶lÃ¼mden aldÄ±ÄŸÄ±nÄ± belirt

YAPMA:
- Genel tavsiyeler verme, spesifik ol
- SpekÃ¼la

syon yapma, sadece dokÃ¼manlara dayanarak cevap ver
- Uzun giriÅŸ paragraflarÄ± yazma, direkt konuya gir
- Ä°ngilizce kelime karÄ±ÅŸtÄ±rma

TÃ¼rkÃ§e dil bilgisi kurallarÄ±na DÄ°KKAT ET. YazÄ±m hatasÄ± yapma."""

        # Context metni oluÅŸtur
        context_parts = []
        
        if context_chunks:
            context_parts.append("ğŸ“š Ä°LGÄ°LÄ° DOKÃœMAN BÄ°LGÄ°LERÄ°:\n")
            for i, chunk in enumerate(context_chunks, 1):
                context_parts.append(f"[Kaynak {i}: {chunk['source']}]")
                context_parts.append(f"{chunk['text']}\n")
        
        if fault_info:
            context_parts.append("\nğŸ”§ ARIZA KODU BÄ°LGÄ°SÄ°:")
            context_parts.append(f"Kod: {fault_info.get('code')}")
            context_parts.append(f"Ä°sim: {fault_info.get('name')}")
            context_parts.append(f"Kategori: {fault_info.get('category')}")
            context_parts.append(f"Ã–nem: {fault_info.get('severity')}\n")
        
        context_text = "\n".join(context_parts) if context_parts else "Not: Ä°lgili dokÃ¼man bulunamadÄ±."
        
        # YapÄ±landÄ±rÄ±lmÄ±ÅŸ user prompt
        user_prompt = f"""SORU: {query}

{context_text}

CEVABINI ÅU FORMATTA VER:

ğŸ“‹ Ã–ZET:
[Tek cÃ¼mle ile sorunun Ã§Ã¶zÃ¼mÃ¼]

ğŸ”§ ADIMLAR:
1. [Ä°lk adÄ±m - spesifik ve uygulanabilir]
2. [Ä°kinci adÄ±m - spesifik ve uygulanabilir]
3. [Devam eden adÄ±mlar...]

âš ï¸ GÃœVENLÄ°K:
[Varsa gÃ¼venlik uyarÄ±larÄ±, yoksa "Standart gÃ¼venlik Ã¶nlemleri yeterli"]

ğŸ“š KAYNAK:
[Hangi dokÃ¼man/bÃ¶lÃ¼mden - eÄŸer dokÃ¼manda yoksa "DokÃ¼manlarda bu bilgi yok" de]"""
        
        # Cevap Ã¼ret (dÃ¼ÅŸÃ¼k temperature = daha tutarlÄ±)
        answer = self.llm.generate(
            prompt=user_prompt, 
            system=system_prompt,
            temperature=0.3,  # DÃ¼ÅŸÃ¼k = deterministik
            top_p=0.9,
            max_tokens=1024
        )
        
        return answer
    
    def save_vector_db(self, path: str):
        """VektÃ¶r DB'yi kaydet"""
        self.vector_store.save(path)


if __name__ == "__main__":
    # Test
    print("RAG Engine Test\n")
    
    # Test chunk'larÄ±
    test_chunks = [
        {
            'text': 'Motor yaÄŸ seviyesi her Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce kontrol edilmelidir. YaÄŸ Ã¶lÃ§Ã¼m Ã§ubuÄŸunu Ã§Ä±karÄ±n, temizleyin ve tekrar takÄ±n.',
            'source': 'test_manual.pdf',
            'chunk_id': 0
        },
        {
            'text': 'RadyatÃ¶r kapaÄŸÄ±nÄ± asla sÄ±cak motorda aÃ§mayÄ±n. SoÄŸutma suyu seviyesi rezervuar iÅŸaretlerinde olmalÄ±dÄ±r.',
            'source': 'test_manual.pdf',
            'chunk_id': 1
        }
    ]
    
    # RAG engine oluÅŸtur
    rag = RAGEngine(llm_model="mistral")
    
    # DokÃ¼manlarÄ± ekle
    rag.add_documents(test_chunks)
    
    # Sorgu
    query = "YaÄŸ seviyesi nasÄ±l kontrol edilir?"
    print(f"\nğŸ“ Sorgu: {query}\n")
    
    # Context al
    context = rag.retrieve_context(query, top_k=2)
    print("ğŸ” Bulunan Context:")
    for c in context:
        print(f"  - {c['source']}: {c['text'][:60]}... (skor: {c['similarity']:.3f})")
    
    # Cevap Ã¼ret (opsiyonel - Ollama gerekli)
    # answer = rag.generate_answer(query)
    # print(f"\nğŸ’¬ Cevap:\n{answer}")
