"""
RAG (Retrieval-Augmented Generation) Motoru

Bu modÃ¼l, vektÃ¶r veritabanÄ± kullanarak dokÃ¼man chunk'larÄ±nÄ± saklar,
sorgu embedding'i oluÅŸturur, en yakÄ±n chunk'larÄ± bulur ve
Ollama ile cevap Ã¼retir.
"""

import os
import pickle
from typing import List, Dict, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import ollama


class Embedder:
    """Metin embedding oluÅŸturma"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        """
        Args:
            model_name: Sentence-transformers model adÄ±
        """
        print(f"ğŸ¤– Embedding modeli yÃ¼kleniyor: {model_name}")
        self.model = SentenceTransformer(model_name)
        print("   âœ“ Model yÃ¼klendi")
    
    def encode(self, texts: List[str]) -> np.ndarray:
        """
        Metinleri embedding'e Ã§evir
        
        Args:
            texts: Metin listesi
        
        Returns:
            Embedding matrisi (n_texts x embedding_dim)
        """
        return self.model.encode(texts, show_progress_bar=True)
    
    def encode_single(self, text: str) -> np.ndarray:
        """Tek bir metni embedding'e Ã§evir"""
        return self.model.encode([text])[0]


class VectorStore:
    """FAISS tabanlÄ± vektÃ¶r veritabanÄ± (basitleÅŸtirilmiÅŸ)"""
    
    def __init__(self, embedding_dim: int = 384):
        """
        Args:
            embedding_dim: Embedding boyutu (all-MiniLM-L6-v2 iÃ§in 384)
        """
        self.embedding_dim = embedding_dim
        self.embeddings = []
        self.chunks = []
        self.index_built = False
    
    def add_documents(self, chunks: List[Dict], embeddings: np.ndarray):
        """
        DokÃ¼manlarÄ± ve embedding'leri ekle
        
        Args:
            chunks: Chunk metadata listesi
            embeddings: Chunk embedding'leri
        """
        if len(chunks) != len(embeddings):
            raise ValueError("Chunk sayÄ±sÄ± embedding sayÄ±sÄ±na eÅŸit olmalÄ±")
        
        self.chunks.extend(chunks)
        self.embeddings.append(embeddings)
        self.index_built = False
    
    def build_index(self):
        """Embedding matrisini oluÅŸtur"""
        if self.embeddings:
            self.embeddings = np.vstack(self.embeddings)
            self.index_built = True
            print(f"âœ“ VektÃ¶r indeksi oluÅŸturuldu: {len(self.chunks)} chunk")
    
    def search(self, query_embedding: np.ndarray, top_k: int = 3) -> List[Dict]:
        """
        En yakÄ±n chunk'larÄ± bul (cosine similarity)
        
        Args:
            query_embedding: Sorgu embedding'i
            top_k: KaÃ§ sonuÃ§ dÃ¶ndÃ¼rÃ¼lecek
        
        Returns:
            En yakÄ±n chunk'lar
        """
        if not self.index_built:
            self.build_index()
        
        if len(self.chunks) == 0:
            return []
        
        # Cosine similarity hesapla
        query_norm = query_embedding / np.linalg.norm(query_embedding)
        embeddings_norm = self.embeddings / np.linalg.norm(
            self.embeddings, axis=1, keepdims=True
        )
        
        similarities = np.dot(embeddings_norm, query_norm)
        
        # En yÃ¼ksek skorlarÄ± al
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            result = self.chunks[idx].copy()
            result['similarity'] = float(similarities[idx])
            results.append(result)
        
        return results
    
    def save(self, path: str):
        """VektÃ¶r DB'yi kaydet"""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        data = {
            'embeddings': self.embeddings,
            'chunks': self.chunks,
            'embedding_dim': self.embedding_dim
        }
        
        with open(path, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"ğŸ’¾ VektÃ¶r DB kaydedildi: {path}")
    
    def load(self, path: str):
        """VektÃ¶r DB'yi yÃ¼kle"""
        if not os.path.exists(path):
            raise FileNotFoundError(f"VektÃ¶r DB bulunamadÄ±: {path}")
        
        with open(path, 'rb') as f:
            data = pickle.load(f)
        
        self.embeddings = data['embeddings']
        self.chunks = data['chunks']
        self.embedding_dim = data['embedding_dim']
        self.index_built = True
        
        print(f"âœ“ VektÃ¶r DB yÃ¼klendi: {len(self.chunks)} chunk")


class OllamaLLM:
    """Ollama LLM entegrasyonu"""
    
    def __init__(self, model: str = "mistral", url: str = "http://localhost:11434"):
        """
        Args:
            model: Ollama model adÄ±
            url: Ollama API URL
        """
        self.model = model
        self.url = url
        self._check_connection()
    
    def _check_connection(self):
        """Ollama baÄŸlantÄ±sÄ±nÄ± kontrol et"""
        try:
            ollama.list()
            print(f"âœ“ Ollama baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±")
        except Exception as e:
            print(f"âš ï¸  Ollama baÄŸlantÄ± hatasÄ±: {e}")
            print("   Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun: ollama serve")
    
    def generate(self, prompt: str, system: str = "") -> str:
        """
        Ollama ile cevap Ã¼ret
        
        Args:
            prompt: KullanÄ±cÄ± prompt'u
            system: Sistem prompt'u
        
        Returns:
            Ãœretilen cevap
        """
        try:
            messages = []
            
            if system:
                messages.append({
                    'role': 'system',
                    'content': system
                })
            
            messages.append({
                'role': 'user',
                'content': prompt
            })
            
            response = ollama.chat(
                model=self.model,
                messages=messages
            )
            
            return response['message']['content']
        
        except Exception as e:
            return f"Ollama hatasÄ±: {str(e)}\n\nOllama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan ve '{self.model}' modelinin yÃ¼klÃ¼ olduÄŸundan emin olun."


class RAGEngine:
    """RAG sistemi ana motoru"""
    
    def __init__(
        self,
        embedding_model: str = "all-MiniLM-L6-v2",
        llm_model: str = "mistral",
        vector_db_path: Optional[str] = None
    ):
        """
        Args:
            embedding_model: Sentence-transformers model
            llm_model: Ollama model
            vector_db_path: VektÃ¶r DB yolu (varsa yÃ¼kle)
        """
        self.embedder = Embedder(model_name=embedding_model)
        self.vector_store = VectorStore(embedding_dim=384)
        self.llm = OllamaLLM(model=llm_model)
        
        # VektÃ¶r DB varsa yÃ¼kle
        if vector_db_path and os.path.exists(vector_db_path):
            self.vector_store.load(vector_db_path)
    
    def add_documents(self, chunks: List[Dict]):
        """
        DokÃ¼manlarÄ± RAG sistemine ekle
        
        Args:
            chunks: Chunk listesi (document_processor'dan)
        """
        if not chunks:
            print("Eklenecek chunk yok")
            return
        
        print(f"\nğŸ”„ {len(chunks)} chunk iÃ§in embedding oluÅŸturuluyor...")
        
        # Embedding oluÅŸtur
        texts = [chunk['text'] for chunk in chunks]
        embeddings = self.embedder.encode(texts)
        
        # VektÃ¶r DB'ye ekle
        self.vector_store.add_documents(chunks, embeddings)
        self.vector_store.build_index()
    
    def retrieve_context(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        Sorguya en yakÄ±n dokÃ¼man parÃ§alarÄ±nÄ± getir
        
        Args:
            query: KullanÄ±cÄ± sorusu
            top_k: KaÃ§ chunk dÃ¶ndÃ¼rÃ¼lecek
        
        Returns:
            En yakÄ±n chunk'lar
        """
        # Query embedding oluÅŸtur
        query_embedding = self.embedder.encode_single(query)
        
        # Benzer chunk'larÄ± bul
        results = self.vector_store.search(query_embedding, top_k=top_k)
        
        return results
    
    def generate_answer(
        self,
        query: str,
        context_chunks: Optional[List[Dict]] = None,
        fault_info: Optional[Dict] = None,
        top_k: int = 3
    ) -> str:
        """
        Sorguya cevap Ã¼ret (RAG)
        
        Args:
            query: KullanÄ±cÄ± sorusu
            context_chunks: Ã–nceden alÄ±nmÄ±ÅŸ context (yoksa otomatik al)
            fault_info: ArÄ±za kodu bilgisi (varsa)
            top_k: KaÃ§ chunk kullanÄ±lacak
        
        Returns:
            Ãœretilen cevap
        """
        # Context yoksa al
        if context_chunks is None:
            context_chunks = self.retrieve_context(query, top_k=top_k)
        
        # Context oluÅŸtur
        context_text = ""
        
        if context_chunks:
            context_text += "Ä°lgili DokÃ¼man Bilgileri:\n\n"
            for i, chunk in enumerate(context_chunks, 1):
                context_text += f"[Kaynak {i}: {chunk['source']}]\n"
                context_text += f"{chunk['text']}\n\n"
        
        if fault_info:
            context_text += f"\nArÄ±za Kodu Bilgisi:\n"
            context_text += f"Kod: {fault_info.get('code')}\n"
            context_text += f"Ä°sim: {fault_info.get('name')}\n"
            context_text += f"Kategori: {fault_info.get('category')}\n"
        
        # Prompt oluÅŸtur
        system_prompt = """Sen bir askeri jeneratÃ¶r bakÄ±m uzmanÄ±sÄ±n. 
Sana verilen teknik dokÃ¼manlar ve arÄ±za kodu bilgilerine dayanarak 
detaylÄ±, pratik ve gÃ¼venli Ã§Ã¶zÃ¼mler Ã¶ner. 
CevaplarÄ±nÄ± TÃ¼rkÃ§e ver ve mÃ¼mkÃ¼nse adÄ±m adÄ±m aÃ§Ä±kla."""
        
        user_prompt = f"""Soru: {query}

{context_text if context_text else "Not: Ä°lgili dokÃ¼man bulunamadÄ±, genel bilgilerinle cevap ver."}

YukarÄ±daki bilgilere dayanarak soruyu cevapla:"""
        
        # Cevap Ã¼ret
        answer = self.llm.generate(prompt=user_prompt, system=system_prompt)
        
        return answer
    
    def save_vector_db(self, path: str):
        """VektÃ¶r DB'yi kaydet"""
        self.vector_store.save(path)


if __name__ == "__main__":
    # Test
    print("RAG Engine Test\n")
    
    # Test chunk'larÄ±
    test_chunks = [
        {
            'text': 'Motor yaÄŸ seviyesi her Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce kontrol edilmelidir. YaÄŸ Ã¶lÃ§Ã¼m Ã§ubuÄŸunu Ã§Ä±karÄ±n, temizleyin ve tekrar takÄ±n.',
            'source': 'test_manual.pdf',
            'chunk_id': 0
        },
        {
            'text': 'RadyatÃ¶r kapaÄŸÄ±nÄ± asla sÄ±cak motorda aÃ§mayÄ±n. SoÄŸutma suyu seviyesi rezervuar iÅŸaretlerinde olmalÄ±dÄ±r.',
            'source': 'test_manual.pdf',
            'chunk_id': 1
        }
    ]
    
    # RAG engine oluÅŸtur
    rag = RAGEngine(llm_model="mistral")
    
    # DokÃ¼manlarÄ± ekle
    rag.add_documents(test_chunks)
    
    # Sorgu
    query = "YaÄŸ seviyesi nasÄ±l kontrol edilir?"
    print(f"\nğŸ“ Sorgu: {query}\n")
    
    # Context al
    context = rag.retrieve_context(query, top_k=2)
    print("ğŸ” Bulunan Context:")
    for c in context:
        print(f"  - {c['source']}: {c['text'][:60]}... (skor: {c['similarity']:.3f})")
    
    # Cevap Ã¼ret (opsiyonel - Ollama gerekli)
    # answer = rag.generate_answer(query)
    # print(f"\nğŸ’¬ Cevap:\n{answer}")
