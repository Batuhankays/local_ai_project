"""
Ollama BaÄŸlantÄ± Testi

Ollama servisinin Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± ve model'in yÃ¼klÃ¼ olduÄŸunu kontrol eder.
"""

import sys
import os

# Proje kÃ¶k dizinini path'e ekle
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))


def test_ollama_connection():
    """Ollama baÄŸlantÄ±sÄ±nÄ± test et"""
    try:
        import ollama
        
        print("ğŸ” Ollama baÄŸlantÄ±sÄ± kontrol ediliyor...\n")
        
        # YÃ¼klÃ¼ modelleri listele
        models = ollama.list()
        
        print("âœ“ Ollama servisine baÄŸlanÄ±ldÄ±")
        print(f"\nğŸ“‹ YÃ¼klÃ¼ modeller:")
        
        if not models.get('models'):
            print("  âš ï¸  HiÃ§ model yÃ¼klÃ¼ deÄŸil!")
            print("\n  Model yÃ¼klemek iÃ§in:")
            print("    ollama pull mistral")
            print("    ollama pull llama2")
            return False
        
        for model in models['models']:
            model_name = model.get('name', 'Bilinmeyen')
            print(f"  â€¢ {model_name}")
        
        print("\nâœ… Ollama hazÄ±r!")
        return True
    
    except ImportError:
        print("âŒ 'ollama' paketi yÃ¼klÃ¼ deÄŸil!")
        print("\n  YÃ¼klemek iÃ§in:")
        print("    pip install ollama")
        return False
    
    except Exception as e:
        print(f"âŒ Ollama baÄŸlantÄ± hatasÄ±: {str(e)}")
        print("\n  Ollama servisini baÅŸlatmak iÃ§in:")
        print("    ollama serve")
        print("\n  veya Ollama'yÄ± yÃ¼kleyin:")
        print("    https://ollama.ai/download")
        return False


def test_ollama_generation():
    """Basit bir prompt testi"""
    try:
        import ollama
        
        print("\n" + "=" * 60)
        print("ğŸ§ª Ollama cevap Ã¼retimi testi")
        print("=" * 60 + "\n")
        
        model = "mistral"
        prompt = "Merhaba! Bu bir test mesajÄ±dÄ±r. KÄ±saca cevap ver."
        
        print(f"Model: {model}")
        print(f"Prompt: {prompt}\n")
        print("Cevap bekleniyor...\n")
        
        response = ollama.chat(
            model=model,
            messages=[
                {'role': 'user', 'content': prompt}
            ]
        )
        
        answer = response['message']['content']
        
        print(f"ğŸ¤– Cevap:\n{answer}\n")
        print("âœ… Cevap Ã¼retimi baÅŸarÄ±lÄ±!")
        
        return True
    
    except Exception as e:
        print(f"âŒ Cevap Ã¼retim hatasÄ±: {str(e)}")
        
        if "model" in str(e).lower():
            print(f"\n  '{model}' modeli yÃ¼klÃ¼ deÄŸil!")
            print(f"\n  YÃ¼klemek iÃ§in:")
            print(f"    ollama pull {model}")
        
        return False


if __name__ == "__main__":
    print("=" * 60)
    print("OLLAMA BAÄLANTI TESTÄ°")
    print("=" * 60 + "\n")
    
    # Test 1: BaÄŸlantÄ±
    connection_ok = test_ollama_connection()
    
    # Test 2: Cevap Ã¼retimi (sadece baÄŸlantÄ± baÅŸarÄ±lÄ±ysa)
    if connection_ok:
        generation_ok = test_ollama_generation()
    else:
        print("\nâš ï¸  BaÄŸlantÄ± baÅŸarÄ±sÄ±z olduÄŸu iÃ§in cevap Ã¼retimi test edilmedi")
        generation_ok = False
    
    # SonuÃ§
    print("\n" + "=" * 60)
    if connection_ok and generation_ok:
        print("âœ… TÃœM TESTLER BAÅARILI!")
        print("\nğŸ¯ Ollama kullanÄ±ma hazÄ±r")
    else:
        print("âŒ BAZI TESTLER BAÅARISIZ")
        print("\nğŸ”§ YukarÄ±daki talimatlarÄ± takip edin")
    print("=" * 60 + "\n")
